{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding How Machines Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = 'The quick brown fox jumps over the lazy dog.'\n",
    "text_2 = 'My dog is quick and can jump over fences.'\n",
    "text_3 = 'Your dog is so lazy that it sleeps all the day.'\n",
    "corpus = [text_1, text_2, text_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 0]\n",
      " [0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0]\n",
      " [1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "vectorizer = text.CountVectorizer(binary=True).fit(corpus)\n",
    "vectorized_text = vectorizer.transform(corpus)\n",
    "print(vectorized_text.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 19, 'quick': 15, 'brown': 2, 'fox': 7, 'jumps': 11, 'over': 14, 'lazy': 12, 'dog': 5, 'my': 13, 'is': 8, 'and': 1, 'can': 3, 'jump': 10, 'fences': 6, 'your': 20, 'so': 17, 'that': 18, 'it': 9, 'sleeps': 16, 'all': 0, 'day': 4}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all': 0, 'and': 1, 'brown': 2, 'can': 3, 'day': 4, 'dog': 5, 'fences': 6, 'fox': 7, 'is': 8, 'it': 9, 'jump': 10, 'jumps': 11, 'lazy': 12, 'my': 13, 'over': 14, 'quick': 15, 'sleeps': 16, 'so': 17, 'that': 18, 'the': 19, 'your': 20}\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "ordered = OrderedDict(sorted(\n",
    "    vectorizer.vocabulary_.items(), \n",
    "    key=lambda x: x[1]))\n",
    "print(dict(ordered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and enhancing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing word counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 1 0 0 2 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "text_4 = 'A black dog just passed by but my dog is brown.'\n",
    "corpus.append(text_4)\n",
    "\n",
    "vectorizer = text.CountVectorizer().fit(corpus)\n",
    "vectorized_text = vectorizer.transform(corpus)\n",
    "print(vectorized_text.todense()[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing weights using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     brown: 0.095\n",
      "       dog: 0.126\n",
      "        my: 0.095\n",
      "        is: 0.077\n",
      "     black: 0.121\n",
      "      just: 0.121\n",
      "    passed: 0.121\n",
      "        by: 0.121\n",
      "       but: 0.121\n",
      "\n",
      "Summed values of a phrase: 1.0\n"
     ]
    }
   ],
   "source": [
    "TfidF = text.TfidfTransformer(norm='l1')\n",
    "tfidf = TfidF.fit_transform(vectorized_text)\n",
    "\n",
    "phrase = 3 # choose a number from 0 to 3\n",
    "total = 0\n",
    "for word in vectorizer.vocabulary_:\n",
    "    pos = vectorizer.vocabulary_[word]\n",
    "    value = list(tfidf.toarray()[phrase])[pos]\n",
    "    if value !=0:\n",
    "        print (\"%10s: %0.3f\" % (word, value))\n",
    "        total += value\n",
    "print ('\\nSummed values of a phrase: %0.1f' % total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maintaining order using n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all the': 0, 'and can': 1, 'black dog': 2, 'brown fox': 3, 'but my': 4, 'by but': 5, 'can jump': 6, 'dog is': 7, 'dog just': 8, 'fox jumps': 9, 'is brown': 10, 'is quick': 11, 'is so': 12, 'it sleeps': 13, 'jump over': 14, 'jumps over': 15, 'just passed': 16, 'lazy dog': 17, 'lazy that': 18, 'my dog': 19, 'over fences': 20, 'over the': 21, 'passed by': 22, 'quick and': 23, 'quick brown': 24, 'sleeps all': 25, 'so lazy': 26, 'that it': 27, 'the day': 28, 'the lazy': 29, 'the quick': 30, 'your dog': 31}\n"
     ]
    }
   ],
   "source": [
    "bigrams = text.CountVectorizer(ngram_range=(2,2))\n",
    "ord_bigrams = OrderedDict(sorted(\n",
    "    bigrams.fit(corpus).vocabulary_.items(), \n",
    "    key=lambda x: x[1]))\n",
    "print(dict(ord_bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'sam', 'swim', 'time']\n",
      "[[1 0 1 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\John\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vocab = ['Sam loves swimming so he swims all the time']\n",
    "vect = text.CountVectorizer(tokenizer=tokenize, \n",
    "                           stop_words='english')\n",
    "vec = vect.fit(vocab)\n",
    "\n",
    "sentence1 = vec.transform(['George loves swimming too!'])\n",
    "\n",
    "print (vec.get_feature_names())\n",
    "print (sentence1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Textual Datasets from the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Beautiful Soup in your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "try:\n",
    "    import urllib2 # Python 2.7.x\n",
    "except:\n",
    "    import urllib.request as urllib2 # Python 3.x\n",
    "\n",
    "wiki = \"https://en.wikipedia.org/wiki/\\\n",
    "List_of_United_States_cities_by_population\"\n",
    "header = {'User-Agent': 'Mozilla/5.0'} \n",
    "query = urllib2.Request(wiki, headers=header)\n",
    "page = urllib2.urlopen(query)\n",
    "soup = BeautifulSoup(page, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 City Population_2017\n",
      "0       New York City      8,622,698\n",
      "\n",
      "1         Los Angeles      3,999,759\n",
      "\n",
      "2             Chicago      2,716,450\n",
      "\n",
      "3             Houston      2,312,717\n",
      "\n",
      "4             Phoenix      1,626,078\n",
      "\n",
      "5        Philadelphia      1,580,863\n",
      "\n",
      "6         San Antonio      1,511,946\n",
      "\n",
      "7           San Diego      1,419,516\n",
      "\n",
      "8              Dallas      1,341,075\n",
      "\n",
      "9            San Jose      1,035,317\n",
      "\n",
      "10             Austin        950,715\n",
      "\n",
      "11       Jacksonville        892,062\n",
      "\n",
      "12      San Francisco        884,363\n",
      "\n",
      "13           Columbus        879,170\n",
      "\n",
      "14         Fort Worth        874,168\n",
      "\n",
      "15       Indianapolis        863,002\n",
      "\n",
      "16          Charlotte        859,035\n",
      "\n",
      "17            Seattle        724,745\n",
      "\n",
      "18             Denver        704,621\n",
      "\n",
      "19   Washington, D.C.        693,972\n",
      "\n",
      "20             Boston        685,094\n",
      "\n",
      "21            El Paso        683,577\n",
      "\n",
      "22            Detroit        673,104\n",
      "\n",
      "23          Nashville        667,560\n",
      "\n",
      "24            Memphis        652,236\n",
      "\n",
      "25           Portland        647,805\n",
      "\n",
      "26      Oklahoma City        643,648\n",
      "\n",
      "27          Las Vegas        641,676\n",
      "\n",
      "28         Louisville        621,349\n",
      "\n",
      "29          Baltimore        611,648\n",
      "\n",
      "..                ...             ...\n",
      "281       Santa Maria        107,014\n",
      "\n",
      "282         Hillsboro        106,894\n",
      "\n",
      "283     Sandy Springs        106,739\n",
      "\n",
      "284           Norwalk        106,084\n",
      "\n",
      "285     Jurupa Valley        106,028\n",
      "\n",
      "286        Lewisville        106,021\n",
      "\n",
      "287           Greeley        105,448\n",
      "\n",
      "288             Davie        105,149\n",
      "\n",
      "289         Green Bay        105,116\n",
      "\n",
      "290             Tyler        104,991\n",
      "\n",
      "291       League City        104,903\n",
      "\n",
      "292           Burbank        104,834\n",
      "\n",
      "293         San Mateo        104,748\n",
      "\n",
      "294     Wichita Falls        104,747\n",
      "\n",
      "295          El Cajon        103,894\n",
      "\n",
      "296            Rialto        103,562\n",
      "\n",
      "297          Lakewood        102,682\n",
      "\n",
      "298            Edison        102,450\n",
      "\n",
      "299         Davenport        102,320\n",
      "\n",
      "300        South Bend        102,245\n",
      "\n",
      "301        Woodbridge        101,965\n",
      "\n",
      "302        Las Cruces        101,712\n",
      "\n",
      "303             Vista        101,568\n",
      "\n",
      "304            Renton        101,379\n",
      "\n",
      "305            Sparks        100,888\n",
      "\n",
      "306           Clinton        100,712\n",
      "\n",
      "307             Allen        100,685\n",
      "\n",
      "308        Tuscaloosa        100,287\n",
      "\n",
      "309        San Angelo        100,119\n",
      "\n",
      "310         Vacaville        100,032\n",
      "\n",
      "\n",
      "[311 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "table = soup.find(\"table\", \n",
    "    { \"class\" : \"wikitable sortable\" })\n",
    "final_table = list()\n",
    "for row in table.findAll('tr'):\n",
    "    cells = row.findAll(\"td\")\n",
    "    if len(cells) >=6:\n",
    "        v1 = cells[1].find(text=True)\n",
    "        v2 = cells[2].find(text=True)\n",
    "        v3 = cells[3].find(text=True)\n",
    "        v4 = cells[4].find(text=True)\n",
    "        v5 = cells[6].findAll(text=True)\n",
    "        final_table.append([v1, v2, v3, v4, v5])\n",
    "cols = ['City','State','Population_2017','Census_2010'\n",
    "        ,'Land_Area_km2']\n",
    "df = pd.DataFrame(final_table, columns=cols)\n",
    "\n",
    "print(df[['City', 'Population_2017']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling problems with raw text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello there!' <class 'bytes'>\n",
      "This is a new string! <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "utf8_string = \"Hello there!\"\n",
    "utf7_string = utf8_string.encode('utf7')\n",
    "print(utf7_string, type(utf7_string))\n",
    "\n",
    "utf7_string = \"This is a new string!\".encode(\"utf7\")\n",
    "utf8_string = utf7_string.decode('utf8')\n",
    "print(utf8_string, type(utf8_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xff\\xfe\\x00\\x00H\\x00\\x00\\x00e\\x00\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00 \\x00\\x00\\x00t\\x00\\x00\\x00h\\x00\\x00\\x00e\\x00\\x00\\x00r\\x00\\x00\\x00e\\x00\\x00\\x00!\\x00\\x00\\x00' <class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "utf8_string = \"Hello there!\"\n",
    "utf32_string = utf8_string.encode('utf32')\n",
    "print(utf32_string, type(utf32_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cp1252'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering Unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a winking face: ðŸ˜‰\n",
      "This is a winking face: ðŸ˜‰\n",
      "This is not a winking face: á½ 9\n"
     ]
    }
   ],
   "source": [
    "uString1 = \"This is a winking face: \\N{WINKING FACE}\"\n",
    "print(uString1)\n",
    "\n",
    "uString2 = \"This is a winking face: \\U0001F609\"\n",
    "print(uString2)\n",
    "\n",
    "uString3 = \"This is not a winking face: \\u1F609\"\n",
    "print(uString3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'This is a winking face: +2D3eCQ-'\n",
      "This is a winking face: +2D3eCQ-\n"
     ]
    }
   ],
   "source": [
    "utf7_string = uString1.encode('utf7')\n",
    "print(utf7_string)\n",
    "\n",
    "uString4 = utf7_string.decode('utf8')\n",
    "print(uString4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'This is a winking face: \\\\N{WINKING FACE}'\n",
      "This is a winking face: \\N{WINKING FACE}\n"
     ]
    }
   ],
   "source": [
    "utf7_string = uString1.encode('ascii', 'namereplace')\n",
    "print(utf7_string)\n",
    "\n",
    "uString4 = utf7_string.decode('utf8', 'replace')\n",
    "print(uString4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing processed text data in sparse matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 2 0]\n",
      " [0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1]\n",
      " [0 0 1 1 1 1 0 0 2 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0]]\n",
      "  (0, 3)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 24)\t2\n",
      "  (1, 1)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 18)\t1\n",
      "  (1, 20)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 16)\t1\n",
      "  (2, 21)\t1\n",
      "  (2, 22)\t1\n",
      "  (2, 23)\t1\n",
      "  (2, 24)\t1\n",
      "  (2, 25)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 8)\t2\n",
      "  (3, 11)\t1\n",
      "  (3, 15)\t1\n",
      "  (3, 17)\t1\n",
      "  (3, 19)\t1\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "full_matrix = vectorized_text.todense()\n",
    "print(full_matrix)\n",
    "\n",
    "sparse_matrix = csr_matrix(full_matrix)\n",
    "print(sparse_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the MovieLens sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "filename = \"ml-20m.zip\"\n",
    "if not os.path.exists(\"ml-20m.zip\"):\n",
    "    url = \"http://files.grouplens.org/datasets/\\\n",
    "movielens/ml-20m.zip\"\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    \n",
    "archive = ZipFile(filename)\n",
    "archive.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000263, 4)\n",
      "   userId  movieId  rating   timestamp\n",
      "0       1        2     3.5  1112486027\n",
      "1       1       29     3.5  1112484676\n",
      "2       1       32     3.5  1112484819\n",
      "3       1       47     3.5  1112484727\n",
      "4       1       50     3.5  1112484580\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_csv(\"ml-20m/ratings.csv\")\n",
    "print(ratings.shape)\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27278, 3)\n",
      "   movieId                               title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                                        genres  \n",
      "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
      "1                   Adventure|Children|Fantasy  \n",
      "2                               Comedy|Romance  \n",
      "3                         Comedy|Drama|Romance  \n",
      "4                                       Comedy  \n"
     ]
    }
   ],
   "source": [
    "names = pd.read_csv(\"ml-20m/movies.csv\")\n",
    "print(names.shape)\n",
    "print(names.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000263, 6)\n",
      "   movieId             title                                       genres  \\\n",
      "0        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy   \n",
      "1        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy   \n",
      "2        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy   \n",
      "3        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy   \n",
      "4        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy   \n",
      "\n",
      "   userId  rating   timestamp  \n",
      "0       3     4.0   944919407  \n",
      "1       6     5.0   858275452  \n",
      "2       8     4.0   833981871  \n",
      "3      10     4.0   943497887  \n",
      "4      11     4.5  1230858821  \n"
     ]
    }
   ],
   "source": [
    "movie_data = pd.merge(names, ratings, on=\"movieId\")\n",
    "print(movie_data.shape)\n",
    "print(movie_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n",
      "Magic Christmas Tree, The (1964)               0.5\n",
      "Vampir (Cuadecuc, vampir) (1971)               0.5\n",
      "Prisoner of Zenda, The (1979)                  0.5\n",
      "Late Great Planet Earth, The (1979)            0.5\n",
      "Last Warrior, The (Last Patrol, The) (2000)    0.5\n",
      "Name: rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(movie_data.groupby('title')['rating'].mean()\n",
    ".sort_values().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scoring and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts: 585\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, \n",
    "    categories = ['misc.forsale'],\n",
    "     remove=('headers', 'footers', 'quotes'), random_state=101)\n",
    "print ('Posts: %i' % len(dataset.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, \n",
    "            min_df=2, stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(dataset.data)\n",
    "from sklearn.decomposition import NMF\n",
    "n_topics = 5\n",
    "nmf = NMF(n_components=n_topics, random_state=101).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "condition excellent asking offer best car old new sale 10 miles 000 tape cd power\n",
      "Topic #2:\n",
      "00 50 dos 20 10 15 cover 1st new 25 price man 40 shipping comics\n",
      "Topic #3:\n",
      "drive hard card floppy monitor meg ram disk motherboard vga modem brand scsi color internal\n",
      "Topic #4:\n",
      "email looking game games send interested mail thanks like edu good want package price list\n",
      "Topic #5:\n",
      "shipping vcr works stereo obo included amp plus great volume unc mathes gibbs radley remotes\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "n_top_words = 15\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "   print (\"Topic #%d:\" % (topic_idx+1),)\n",
    "   print (\" \".join([feature_names[i] for i in \n",
    "                    topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1075 1459  632 2463  740  888 2476 2415 2987   10 2305    1 3349  923\n",
      " 2680]\n"
     ]
    }
   ],
   "source": [
    "print (nmf.components_[0,:].argsort()[:-n_top_words-1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.get_feature_names()[1337])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing reviews from e-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting in C:\\Users\\John\\DSPD\n",
      "\tunzipping C:\\Users\\John\\DSPD\\amazon_cells_labelled.txt\n",
      "\tunzipping C:\\Users\\John\\DSPD\\imdb_labelled.txt\n",
      "\tunzipping C:\\Users\\John\\DSPD\\readme.txt\n",
      "\tunzipping C:\\Users\\John\\DSPD\\yelp_labelled.txt\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as urllib2\n",
    "import requests, io, os, zipfile\n",
    "\n",
    "UCI_url = 'https://archive.ics.uci.edu/ml/\\\n",
    "machine-learning-databases/00331/sentiment%20\\\n",
    "labelled%20sentences.zip'\n",
    "\n",
    "response = requests.get(UCI_url)\n",
    "compressed_file = io.BytesIO(response.content)\n",
    "z = zipfile.ZipFile(compressed_file)\n",
    "print ('Extracting in %s' %  os.getcwd())\n",
    "for name in z.namelist():\n",
    "    filename = name.split('/')[-1]\n",
    "    nameOK = ('MACOSX' not in name and '.DS' not in name)\n",
    "    if filename and nameOK:\n",
    "            newfile = os.path.join(os.getcwd(), \n",
    "                               os.path.basename(filename))\n",
    "            with open(newfile, 'wb') as f:\n",
    "                f.write(z.read(name))\n",
    "            print ('\\tunzipping %s' % newfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "dataset = 'imdb_labelled.txt'\n",
    "data = pd.read_csv(dataset, header=None, sep=r\"\\t\",\n",
    "                   engine='python')\n",
    "data.columns = ['review','sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "corpus, test_corpus, y, yt = train_test_split(\n",
    "    data.ix[:,0], data.ix[:,1], \n",
    "    test_size=0.25, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "vectorizer = text.CountVectorizer(ngram_range=(1,2), \n",
    "                    stop_words='english').fit(corpus)\n",
    "TfidF = text.TfidfTransformer()\n",
    "X = TfidF.fit_transform(vectorizer.transform(corpus))\n",
    "Xt = TfidF.transform(vectorizer.transform(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid = {'C': [0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "clf = GridSearchCV(LinearSVC(loss='hinge', \n",
    "                    random_state=101), param_grid)\n",
    "clf = clf.fit(X, y)\n",
    "print (\"Best parameters: %s\" % clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achieved accuracy: 0.816\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "solution = clf.predict(Xt)\n",
    "print(\"Achieved accuracy: %0.3f\" % \n",
    "      accuracy_score(yt, solution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601    There is simply no excuse for something this p...\n",
      "32     This is the kind of money that is wasted prope...\n",
      "887    At any rate this film stinks, its not funny, a...\n",
      "668    Speaking of the music, it is unbearably predic...\n",
      "408         It really created a unique feeling though.  \n",
      "413         The camera really likes her in this movie.  \n",
      "138    I saw \"Mirrormask\" last night and it was an un...\n",
      "132    This was a poor remake of \"My Best Friends Wed...\n",
      "291                               Rating: 1 out of 10.  \n",
      "904    I'm so sorry but I really can't recommend it t...\n",
      "410    A world better than 95% of the garbage in the ...\n",
      "55     But I recommend waiting for their future effor...\n",
      "826    The film deserves strong kudos for taking this...\n",
      "100            I don't think you will be disappointed.  \n",
      "352                                    It is shameful.  \n",
      "171    This movie now joins Revenge of the Boogeyman ...\n",
      "814    You share General Loewenhielm's exquisite joy ...\n",
      "218    It's this pandering to the audience that sabot...\n",
      "168    Still, I do like this movie for it's empowerme...\n",
      "479                     Of course, the acting is blah.  \n",
      "31                      Waste your money on this game.  \n",
      "805    The only place good for this film is in the ga...\n",
      "127    My only problem is I thought the actor playing...\n",
      "613                                       Go watch it!  \n",
      "764                      This movie is also revealing.  \n",
      "107    I love Lane, but I've never seen her in a movi...\n",
      "674    Tom Wilkinson broke my heart at the end... and...\n",
      "30     There are massive levels, massive unlockable c...\n",
      "667                                    It is not good.  \n",
      "823    I struggle to find anything bad to say about i...\n",
      "739         What on earth is Irons doing in this film?  \n",
      "185                              Highly unrecommended.  \n",
      "621    A mature, subtle script that suggests and occa...\n",
      "462    Considering the relations off screen between T...\n",
      "595    Easily, none other cartoon made me laugh in a ...\n",
      "8                                   A bit predictable.  \n",
      "446    I like Armand Assante & my cable company's sum...\n",
      "449    I won't say any more - I don't like spoilers, ...\n",
      "715    Im big fan of RPG games too, but this movie, i...\n",
      "241    This would not even be good as a made for TV f...\n",
      "471    At no point in the proceedings does it look re...\n",
      "481    And, FINALLY, after all that, we get to an end...\n",
      "104                           Too politically correct.  \n",
      "522    Rating: 0/10 (Grade: Z) Note: The Show Is So B...\n",
      "174               This film has no redeeming features.  \n",
      "491    This movie creates its own universe, and is fa...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus[yt!=solution])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
